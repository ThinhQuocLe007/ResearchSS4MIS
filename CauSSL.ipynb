{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import random \n",
    "import time \n",
    "import logging \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "from utils.params import params \n",
    "from utils.losses import DiceLoss, softmax_mse_loss, softmax_kl_loss, l_correlation_cos_mean\n",
    "from networks.utils import BCP_net, get_current_consistency_weight, update_ema_variable\n",
    "from dataset.basedataset import BaseDataset\n",
    "from dataset.utils import TwoStreamBatchSampler, RandomGenerator, patients_to_slices\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ACDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data in used: 10.0%\n",
      "X.shape = torch.Size([24, 1, 256, 256])\n",
      "Y.shape = torch.Size([24, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "train_db = BaseDataset(\n",
    "    root_path= args.root_dir, \n",
    "    split= 'train', \n",
    "    transform= transforms.Compose([RandomGenerator(args.patch_size)])\n",
    ")\n",
    "\n",
    "val_db = BaseDataset(\n",
    "    root_path= args.root_dir, \n",
    "    split= 'val'\n",
    ")\n",
    "\n",
    "\n",
    "# split to labeled and unlabeled dataset \n",
    "labeled_slices = patients_to_slices(args.root_dir, args.label_num) \n",
    "label_ratio = round(labeled_slices / len(train_db), 1)* 100\n",
    "print(f'Number of labeled data in used: {label_ratio}%')\n",
    "labeled_idxs = list(range(0, labeled_slices) )\n",
    "unlabeled_idxs = list(range(labeled_slices ,len(train_db)))\n",
    "batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, args.batch_size, args.batch_size - args.labeled_bs)\n",
    "\n",
    "# Create dataloader\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(args.seed + worker_id)\n",
    " \n",
    "trainloader = DataLoader(train_db, batch_sampler= batch_sampler, num_workers= 4, pin_memory= True, worker_init_fn= worker_init_fn)\n",
    "valloader = DataLoader(val_db, batch_size= 1, shuffle= False, num_workers=1)\n",
    "\n",
    "# Check \n",
    "dataiter = iter(trainloader) \n",
    "sampled_batch = next(dataiter) \n",
    "volume_image, volume_label = sampled_batch['image'], sampled_batch['label']\n",
    "volume_image, volume_label = volume_image.cuda(), volume_label.cuda()\n",
    "labeled_volume_batch = volume_image[ : args.labeled_bs]\n",
    "unlabeled_volume_batch = volume_image[args.labeled_bs :]\n",
    "print(f'X.shape = {volume_image.shape}')\n",
    "print(f'Y.shape = {volume_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_vector(nn.Module): \n",
    "    \"\"\"\n",
    "    Implement: \n",
    "        - Initialize the linear transform matrix. G.shape = (ndim, ndim) \n",
    "        - Apply Gb to performce linear transform \n",
    "    Formula: \n",
    "        qb.shape = G * w --> (ndim, k)\n",
    "        w.shape = (ndim, k)\n",
    "        G.shape = (ndim, ndim) \n",
    "    \"\"\"\n",
    "    def __init__(self, ndim): \n",
    "        super(Linear_vector, self).__init__()\n",
    "        self.ndim = ndim \n",
    "        self.params = Parameter(torch.Tensor(self.ndim, self.ndim), requires_grad= True) # Linear transform matrix\n",
    "        self.ratio_init = 1e-3 \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" \n",
    "        Initialize for Standard Scaler \n",
    "        mean = 0 \n",
    "        std = ratio_init \n",
    "        \"\"\" \n",
    "        for param in self.params: \n",
    "            param.data.normal_(0, self.ratio_init)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        result = torch.mm(self.params, x) # dot product  \n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "ndim = 64 \n",
    "k = 10 \n",
    "w1 = torch.randn(ndim, k) \n",
    "w2 = torch.randn(ndim, k)\n",
    "linear_trans1 = Linear_vector(ndim= 64)\n",
    "\n",
    "q21 = linear_trans1(w2) \n",
    "print(q21.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Understand Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss_fn = DiceLoss(n_classes= 4) \n",
    "def supervised_loss(outputs, target, alpha= 0.5): \n",
    "    \"\"\"\n",
    "    Comptute supervised loss for CauSSL (on labeled data only)\n",
    "    supervised = 0.5 ( CE + DICE )\n",
    "    \"\"\"\n",
    "    # Compute CELoss \n",
    "    target = target.long() \n",
    "    loss_ce = F.cross_entropy(outputs, target) \n",
    "\n",
    "    # Compute DiceLoss \n",
    "    loss_dice = dice_loss_fn(outputs, target.unsqueeze(1)) \n",
    "\n",
    "    loss = alpha * ( loss_ce + loss_dice)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                | 5/455 [00:19<28:51,  3.85s/it]/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/autograd/graph.py:769: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_28613/2124750920.py\", line 115, in <module>\n",
      "    icm_loss2 = l_correlation_cos_mean(model2, model1, linear_params2)\n",
      "  File \"/home/lequocthinh/Desktop/pythonCode/SSL4MIS/BCP/Medical_Seg_Semi/utils/losses.py\", line 114, in l_correlation_cos_mean\n",
      "    out = linear_params1[count](w2) # apply linear transform to w2\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_28613/4203513046.py\", line 28, in forward\n",
      "    result = torch.mm(self.params, x) # dot product\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  1%|▎                                | 5/455 [00:22<33:45,  4.50s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 144]] is at version 62; expected version 61 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m optimizer1\u001b[38;5;241m.\u001b[39mstep() \n\u001b[1;32m    127\u001b[0m optimizer2\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m--> 128\u001b[0m \u001b[43mloss2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    129\u001b[0m optimizer2\u001b[38;5;241m.\u001b[39mstep() \n\u001b[1;32m    131\u001b[0m iter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 144]] is at version 62; expected version 61 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "num_classes = args.num_classes\n",
    "base_lr = args.base_lr \n",
    "labeled_bs = args.labeled_bs\n",
    "max_iterations = args.max_iteration\n",
    "snapshot_path ='causalmodel'\n",
    "\n",
    "# create 2 model with the same architecure\n",
    "model1 = BCP_net(in_chns=1, class_num= 4) \n",
    "model2 = BCP_net(in_chns=1, class_num= 4) \n",
    "optimizer1 = optim.SGD(model1.parameters(), base_lr,  momentum= 0.9, weight_decay= 1e-4)\n",
    "optimizer2 = optim.SGD(model2.parameters(), base_lr, momentum= 0.9, weight_decay= 1e-4)\n",
    "\n",
    "model1.train() \n",
    "model2.train() \n",
    "\n",
    "# Initialize linear transform matrix (vector)\n",
    "linear_params1 = [] \n",
    "linear_params2 = [] \n",
    "count = 0\n",
    "for name, parameters in model1.named_parameters(): \n",
    "    if 'conv' in name and 'weight' in name: \n",
    "        if len(parameters.shape) == 4: \n",
    "            count += 1 \n",
    "            outdim = parameters.shape[0] \n",
    "            linear_params1.append(Linear_vector(outdim))\n",
    "            linear_params2.append(Linear_vector(outdim))\n",
    "\n",
    "# Convert from list to torch\n",
    "linear_params1 = nn.ModuleList(linear_params1)\n",
    "linear_params2 = nn.ModuleList(linear_params2)    \n",
    "linear_params1 = linear_params1.cuda() \n",
    "linear_params2 = linear_params2.cuda()\n",
    "\n",
    "linear_optimizer1 = optim.Adam(linear_params1.parameters(), 2e-2) # Need consider about this hyper-parameters\n",
    "linear_optimizer2 = optim.Adam(linear_params2.parameters(), 2e-2)\n",
    "\n",
    "if args.consistency_type == 'mse': \n",
    "    consistency_criterion = softmax_mse_loss\n",
    "elif args.consistency_type == 'kl': \n",
    "    consistency_criterion = softmax_kl_loss\n",
    "else: \n",
    "    assert False, args.consistency_type\n",
    "\n",
    "# Training process - Cross Pseudo Supervision FrameWork \n",
    "writer = SummaryWriter() \n",
    "logging.info(f'{len(trainloader)} per epoch')\n",
    "\n",
    "iter_num = 0 \n",
    "iter_num_max = 0 \n",
    "max_epoch = max_iterations // len(trainloader) + 1 \n",
    "lr_ = base_lr \n",
    "model1.train() \n",
    "model2.train() \n",
    "for epoch_num in tqdm(range(max_epoch), ncols=70): \n",
    "    time1 = time.time() \n",
    "    for i_batch, sampled_batch in enumerate(trainloader): \n",
    "        time2 = time.time() \n",
    "\n",
    "        # Update linear transform matrix periodly \n",
    "        if iter_num > args.start_step1 and iter_num % args.min_step == 0: \n",
    "            icm_loss1 = -l_correlation_cos_mean(model1, model2, linear_params1)\n",
    "            icm_loss2 = -l_correlation_cos_mean(model2, model1, linear_params2)\n",
    "\n",
    "            linear_optimizer1.zero_grad() \n",
    "            linear_optimizer2.zero_grad() \n",
    "\n",
    "            icm_loss1.backward() \n",
    "            icm_loss2.backward() \n",
    "            linear_optimizer1.step() \n",
    "            linear_optimizer2.step() \n",
    "\n",
    "            iter_num_max += 1 \n",
    "\n",
    "            writer.add_scalar('loss/icm_loss1_max', -icm_loss1, iter_num_max)\n",
    "            writer.add_scalar('loss/icm_loss2_max', -icm_loss2, iter_num_max)\n",
    "\n",
    "        # Model prediction        \n",
    "        volume_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "        volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda() \n",
    "        unlabeled_batch = volume_batch[labeled_bs :]\n",
    "\n",
    "        outputs1 = model1(volume_batch)\n",
    "        outputs2 = model2(volume_batch) \n",
    "        # Supervised loss \n",
    "        loss_seg1 = F.cross_entropy(outputs1[: labeled_bs], label_batch[: labeled_bs].long())\n",
    "        loss_seg2 = F.cross_entropy(outputs2[: labeled_bs], label_batch[: labeled_bs].long())\n",
    "\n",
    "        outputs_soft1 = F.softmax(outputs1, dim= 1) \n",
    "        outputs_soft2 = F.softmax(outputs2, dim=1) \n",
    "\n",
    "        loss_dice1 = dice_loss_fn(outputs_soft1[: labeled_bs], label_batch[: labeled_bs].unsqueeze(1))\n",
    "        loss_dice2 = dice_loss_fn(outputs2[: labeled_bs], label_batch[: labeled_bs].unsqueeze(1))\n",
    "\n",
    "        supervised_loss1 = 0.5 * (loss_seg1 + loss_dice1) \n",
    "        supervised_loss2 = 0.5 * (loss_seg2 + loss_dice2) \n",
    "\n",
    "        # Consistency loss \n",
    "        consistency_weight = get_current_consistency_weight(args, iter_num // 150) \n",
    "        pseudo_outputs1 = torch.argmax(outputs_soft1[labeled_bs:].detach(), dim= 1, keepdim= False)\n",
    "        pseudo_outputs2 = torch.argmax(outputs_soft2[labeled_bs:].detach(), dim=1, keepdim= False)\n",
    "\n",
    "        if iter_num > args.thres_iteration: # Only use CPS after 400 iterations \n",
    "            consistency_dist1 = F.cross_entropy(outputs1[labeled_bs:], pseudo_outputs2)\n",
    "            consistency_dist2 = F.cross_entropy(outputs2[labeled_bs:], pseudo_outputs1)\n",
    "        else: \n",
    "            consistency_dist1 = torch.tensor(0.0, device=outputs1.device, requires_grad=True)\n",
    "            consistency_dist2 = torch.tensor(0.0, device=outputs2.device, requires_grad=True)\n",
    "\n",
    "        \n",
    "        consistency_loss1 = consistency_weight * 0.3 * consistency_dist1 # what problem here ??? \n",
    "        consistency_loss2 = consistency_weight * 0.3 * consistency_dist2 \n",
    "\n",
    "        if iter_num > args.start_step2 and iter_num_max > 0: \n",
    "            icm_loss1 = l_correlation_cos_mean(model1, model2, linear_params1)\n",
    "            icm_loss2 = l_correlation_cos_mean(model2, model1, linear_params2)\n",
    "        else: \n",
    "            icm_loss1 = torch.tensor(0.0, device=outputs1.device, requires_grad=True)\n",
    "            icm_loss2 = torch.tensor(0.0, device=outputs2.device, requires_grad=True)\n",
    "        \n",
    "        loss1 = supervised_loss1 + consistency_loss1 + args.cofficient * icm_loss1 \n",
    "        loss2 = supervised_loss2 + consistency_loss2 + args.cofficient * icm_loss2\n",
    "\n",
    "        optimizer1.zero_grad() \n",
    "        loss1.backward() \n",
    "        optimizer1.step() \n",
    "\n",
    "        optimizer2.zero_grad() \n",
    "        loss2.backward() \n",
    "        optimizer2.step() \n",
    "\n",
    "        iter_num += 1 \n",
    "        writer.add_scalar('lr', lr_, iter_num)\n",
    "        writer.add_scalar('loss/loss1', loss1, iter_num)\n",
    "        writer.add_scalar('train/consistency_loss1', consistency_loss1, iter_num)\n",
    "        writer.add_scalar('train/consistency_dist1', consistency_dist1, iter_num)\n",
    "        writer.add_scalar('loss/loss2', loss2, iter_num)\n",
    "        writer.add_scalar('train/consistency_loss2', consistency_loss2, iter_num)\n",
    "        writer.add_scalar('train/consistency_dist2', consistency_dist2, iter_num)\n",
    "        writer.add_scalar('train/consistency_weight', consistency_weight, iter_num)\n",
    "\n",
    "        writer.add_scalar('loss/icm_loss1_min', icm_loss1, iter_num)\n",
    "        writer.add_scalar('loss/icm_loss2_min', icm_loss2, iter_num)\n",
    "\n",
    "        ## change lr\n",
    "        if iter_num % 2500 == 0:\n",
    "            lr_ = base_lr * 0.1 ** (iter_num // 2500)\n",
    "            for param_group in optimizer1.param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "            for param_group in optimizer2.param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "        if iter_num % 1000 == 0:\n",
    "            save_mode_path = os.path.join(snapshot_path, 'iter1_' + str(iter_num) + '.pth')\n",
    "            torch.save(model1.state_dict(), save_mode_path)\n",
    "            logging.info(\"save model to {}\".format(save_mode_path))\n",
    "            save_mode_path = os.path.join(snapshot_path, 'iter2_' + str(iter_num) + '.pth')\n",
    "            torch.save(model2.state_dict(), save_mode_path)\n",
    "            logging.info(\"save model to {}\".format(save_mode_path))\n",
    "\n",
    "        if iter_num >= max_iterations:\n",
    "            break\n",
    "        time1 = time.time()\n",
    "    if iter_num >= max_iterations:\n",
    "        break\n",
    "save_mode_path = os.path.join(snapshot_path, 'iter1_'+str(max_iterations)+'.pth')\n",
    "torch.save(model1.state_dict(), save_mode_path)\n",
    "logging.info(\"save model to {}\".format(save_mode_path))\n",
    "save_mode_path = os.path.join(snapshot_path, 'iter2_'+str(max_iterations)+'.pth')\n",
    "torch.save(model2.state_dict(), save_mode_path)\n",
    "logging.info(\"save model to {}\".format(save_mode_path))\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
