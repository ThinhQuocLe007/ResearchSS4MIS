{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import random \n",
    "import time \n",
    "import logging \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "from utils.params import params \n",
    "from utils.losses import DiceLoss, softmax_mse_loss, softmax_kl_loss, l_correlation_cos_mean\n",
    "from networks.utils import BCP_net, get_current_consistency_weight, update_ema_variable\n",
    "from dataset.basedataset import BaseDataset\n",
    "from dataset.utils import TwoStreamBatchSampler, RandomGenerator, patients_to_slices\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ACDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data in used: 10.0%\n",
      "X.shape = torch.Size([24, 1, 256, 256])\n",
      "Y.shape = torch.Size([24, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "train_db = BaseDataset(\n",
    "    root_path= args.root_dir, \n",
    "    split= 'train', \n",
    "    transform= transforms.Compose([RandomGenerator(args.patch_size)])\n",
    ")\n",
    "\n",
    "val_db = BaseDataset(\n",
    "    root_path= args.root_dir, \n",
    "    split= 'val'\n",
    ")\n",
    "\n",
    "\n",
    "# split to labeled and unlabeled dataset \n",
    "labeled_slices = patients_to_slices(args.root_dir, args.label_num) \n",
    "label_ratio = round(labeled_slices / len(train_db), 1)* 100\n",
    "print(f'Number of labeled data in used: {label_ratio}%')\n",
    "labeled_idxs = list(range(0, labeled_slices) )\n",
    "unlabeled_idxs = list(range(labeled_slices ,len(train_db)))\n",
    "batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, args.batch_size, args.batch_size - args.labeled_bs)\n",
    "\n",
    "# Create dataloader\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(args.seed + worker_id)\n",
    " \n",
    "trainloader = DataLoader(train_db, batch_sampler= batch_sampler, num_workers= 4, pin_memory= True, worker_init_fn= worker_init_fn)\n",
    "valloader = DataLoader(val_db, batch_size= 1, shuffle= False, num_workers=1)\n",
    "\n",
    "# Check \n",
    "dataiter = iter(trainloader) \n",
    "sampled_batch = next(dataiter) \n",
    "volume_image, volume_label = sampled_batch['image'], sampled_batch['label']\n",
    "volume_image, volume_label = volume_image.cuda(), volume_label.cuda()\n",
    "labeled_volume_batch = volume_image[ : args.labeled_bs]\n",
    "unlabeled_volume_batch = volume_image[args.labeled_bs :]\n",
    "print(f'X.shape = {volume_image.shape}')\n",
    "print(f'Y.shape = {volume_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_vector(nn.Module): \n",
    "    \"\"\"\n",
    "    Implement: \n",
    "        - Initialize the linear transform matrix. G.shape = (ndim, ndim) \n",
    "        - Apply Gb to performce linear transform \n",
    "    Formula: \n",
    "        qb.shape = G * w --> (ndim, k)\n",
    "        w.shape = (ndim, k)\n",
    "        G.shape = (ndim, ndim) \n",
    "    \"\"\"\n",
    "    def __init__(self, ndim): \n",
    "        super(Linear_vector, self).__init__()\n",
    "        self.ndim = ndim \n",
    "        self.params = Parameter(torch.Tensor(self.ndim, self.ndim)) # Linear transform matrix\n",
    "        self.ratio_init = 0.3 \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" \n",
    "        Initialize for Standard Scaler \n",
    "        mean = 0 \n",
    "        std = ratio_init \n",
    "        \"\"\" \n",
    "        for param in self.params: \n",
    "            param.data.normal_(0, self.ratio_init)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        result = torch.mm(self.params, x) # dot product  \n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "ndim = 64 \n",
    "k = 10 \n",
    "w1 = torch.randn(ndim, k) \n",
    "w2 = torch.randn(ndim, k)\n",
    "linear_trans1 = Linear_vector(ndim= 64)\n",
    "\n",
    "q21 = linear_trans1(w2) \n",
    "print(q21.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Understand Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss_fn = DiceLoss(n_classes= 4) \n",
    "def supervised_loss(outputs, target, alpha= 0.5): \n",
    "    \"\"\"\n",
    "    Comptute supervised loss for CauSSL (on labeled data only)\n",
    "    supervised = 0.5 ( CE + DICE )\n",
    "    \"\"\"\n",
    "    # Compute CELoss \n",
    "    target = target.long() \n",
    "    loss_ce = F.cross_entropy(outputs, target) \n",
    "\n",
    "    # Compute DiceLoss \n",
    "    loss_dice = dice_loss_fn(outputs, target.unsqueeze(1)) \n",
    "\n",
    "    loss = alpha * ( loss_ce + loss_dice)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████▌   | 9/10 [00:29<00:03,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "num_classes = args.num_classes\n",
    "base_lr = args.base_lr \n",
    "labeled_bs = args.labeled_bs\n",
    "max_iterations = args.max_iteration\n",
    "snapshot_path ='causalmodel'\n",
    "\n",
    "# create 2 model with the same architecure\n",
    "model1 = BCP_net(in_chns=1, class_num= 4) \n",
    "model2 = BCP_net(in_chns=1, class_num= 4) \n",
    "optimizer1 = optim.SGD(model1.parameters(), base_lr,  momentum= 0.9, weight_decay= 1e-4)\n",
    "optimizer2 = optim.SGD(model2.parameters(), base_lr, momentum= 0.9, weight_decay= 1e-4)\n",
    "\n",
    "model1.train() \n",
    "model2.train() \n",
    "\n",
    "# Initialize linear transform matrix (vector)\n",
    "linear_params1 = [] \n",
    "linear_params2 = [] \n",
    "count = 0\n",
    "for name, parameters in model1.named_parameters(): \n",
    "    if 'conv' in name and 'weight' in name: \n",
    "        if len(parameters.shape) == 4: \n",
    "            count += 1 \n",
    "            outdim = parameters.shape[0] \n",
    "            linear_params1.append(Linear_vector(outdim))\n",
    "            linear_params2.append(Linear_vector(outdim))\n",
    "\n",
    "# Convert from list to torch\n",
    "linear_params1 = nn.ModuleList(linear_params1)\n",
    "linear_params2 = nn.ModuleList(linear_params2)    \n",
    "linear_params1 = linear_params1.cuda() \n",
    "linear_params2 = linear_params2.cuda()\n",
    "\n",
    "linear_optimizer1 = optim.Adam(linear_params1.parameters(), 2e-2) # Need consider about this hyper-parameters\n",
    "linear_optimizer2 = optim.Adam(linear_params2.parameters(), 2e-2)\n",
    "\n",
    "if args.consistency_type == 'mse': \n",
    "    consistency_criterion = softmax_mse_loss\n",
    "elif args.consistency_type == 'kl': \n",
    "    consistency_criterion = softmax_kl_loss\n",
    "else: \n",
    "    assert False, args.consistency_type\n",
    "\n",
    "# Training process - Cross Pseudo Supervision FrameWork \n",
    "writer = SummaryWriter() \n",
    "logging.info(f'{len(trainloader)} per epoch')\n",
    "\n",
    "iter_num = 0 \n",
    "iter_num_max = 0 \n",
    "max_epoch = max_iterations // len(trainloader) + 1 \n",
    "lr_ = base_lr \n",
    "model1.train() \n",
    "model2.train() \n",
    "for epoch_num in tqdm(range(max_epoch), ncols=70): \n",
    "    time1 = time.time() \n",
    "    for i_batch, sampled_batch in enumerate(trainloader): \n",
    "        time2 = time.time() \n",
    "\n",
    "        # Update linear transform matrix periodly \n",
    "        if iter_num > args.start_step1 and iter_num % args.min_step == 0: \n",
    "            icm_loss1 = -l_correlation_cos_mean(model1, model2, linear_params1)\n",
    "            icm_loss2 = -l_correlation_cos_mean(model2, model1, linear_params2)\n",
    "\n",
    "            linear_optimizer1.zero_grad() \n",
    "            linear_optimizer2.zero_grad() \n",
    "\n",
    "            icm_loss1.backward() \n",
    "            icm_loss2.backward() \n",
    "            linear_optimizer1.step() \n",
    "            linear_optimizer2.step() \n",
    "\n",
    "            iter_num_max += 1 \n",
    "\n",
    "            writer.add_scalar('loss/icm_loss1_max', -icm_loss1, iter_num_max)\n",
    "            writer.add_scalar('loss/icm_loss2_max', -icm_loss2, iter_num_max)\n",
    "\n",
    "        # Supervised section         \n",
    "        volume_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "        volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda() \n",
    "        unlabeled_batch = volume_batch[labeled_bs :]\n",
    "\n",
    "        outputs1 = model1(volume_batch)\n",
    "        outputs2 = model2(volume_batch) \n",
    "\n",
    "        outputs_soft1 = F.softmax(outputs1, dim=1) \n",
    "        outputs_soft2 = F.softmax(outputs2, dim=1)\n",
    "\n",
    "        supervised_loss1 = supervised_loss(outputs1[: labeled_bs], label_batch[: labeled_bs])\n",
    "        supervised_loss2 = supervised_loss(outputs2[: labeled_bs], label_batch[: labeled_bs])\n",
    "\n",
    "        # Consistency section \n",
    "        consistency_weight = get_current_consistency_weight(args, iter_num // 150) \n",
    "        pseudo_outputs1 = torch.argmax(outputs_soft1[labeled_bs:].detach(), dim= 1, keepdim=True)\n",
    "        pseudo_outputs2 = torch.argmax(outputs_soft2[labeled_bs:].detach(), dim=1, keepdim= True)\n",
    "\n",
    "        if iter_num > args.thres_iteration: # Only use CPS after 400 iterations \n",
    "            consistency_dist1 = F.cross_entropy(outputs1[labeled_bs:], pseudo_outputs2)\n",
    "            consistency_dist2 = F.cross_entropy(outputs2[labeled_bs:], pseudo_outputs2)\n",
    "        else: \n",
    "            consistency_dist1 = 0 \n",
    "            consistency_dist2 = 0 \n",
    "        \n",
    "        consistency_loss1 = consistency_weight * 0.3 * consistency_dist1 # what problem here ??? \n",
    "        consistency_loss2 = consistency_weight * 0.3 * consistency_dist2 \n",
    "\n",
    "        if iter_num > args.start_step2 and iter_num_max > 0: \n",
    "            icm_loss1 = l_correlation_cos_mean(model1, model2, linear_params1)\n",
    "            icm_loss2 = l_correlation_cos_mean(model2, model1, linear_params2)\n",
    "        else: \n",
    "            icm_loss1 = 0 \n",
    "            icm_loss2 = 0 \n",
    "        \n",
    "        loss1 = supervised_loss1 + consistency_loss1 + args.cofficient * icm_loss1 \n",
    "        loss2 = supervised_loss2 + consistency_loss2 + args.cofficient * icm_loss2\n",
    "\n",
    "        optimizer1.zero_grad() \n",
    "        loss1.backward() \n",
    "        optimizer1.step() \n",
    "\n",
    "        optimizer2.zero_grad() \n",
    "        loss2.backward() \n",
    "        optimizer2.step() \n",
    "\n",
    "        iter_num += 1 \n",
    "        writer.add_scalar('lr', lr_, iter_num)\n",
    "        writer.add_scalar('loss/loss1', loss1, iter_num)\n",
    "        writer.add_scalar('train/consistency_loss1', consistency_loss1, iter_num)\n",
    "        writer.add_scalar('train/consistency_dist1', consistency_dist1, iter_num)\n",
    "        writer.add_scalar('loss/loss2', loss2, iter_num)\n",
    "        writer.add_scalar('train/consistency_loss2', consistency_loss2, iter_num)\n",
    "        writer.add_scalar('train/consistency_dist2', consistency_dist2, iter_num)\n",
    "        writer.add_scalar('train/consistency_weight', consistency_weight, iter_num)\n",
    "\n",
    "        writer.add_scalar('loss/icm_loss1_min', icm_loss1, iter_num)\n",
    "        writer.add_scalar('loss/icm_loss2_min', icm_loss2, iter_num)\n",
    "\n",
    "        ## change lr\n",
    "        if iter_num % 2500 == 0:\n",
    "            lr_ = base_lr * 0.1 ** (iter_num // 2500)\n",
    "            for param_group in optimizer1.param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "            for param_group in optimizer2.param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "        if iter_num % 10 == 0:\n",
    "            save_mode_path = os.path.join(snapshot_path, 'iter1_' + str(iter_num) + '.pth')\n",
    "            torch.save(model1.state_dict(), save_mode_path)\n",
    "            logging.info(\"save model to {}\".format(save_mode_path))\n",
    "            save_mode_path = os.path.join(snapshot_path, 'iter2_' + str(iter_num) + '.pth')\n",
    "            torch.save(model2.state_dict(), save_mode_path)\n",
    "            logging.info(\"save model to {}\".format(save_mode_path))\n",
    "\n",
    "        if iter_num >= max_iterations:\n",
    "            break\n",
    "        time1 = time.time()\n",
    "    if iter_num >= max_iterations:\n",
    "        break\n",
    "    save_mode_path = os.path.join(snapshot_path, 'iter1_'+str(max_iterations)+'.pth')\n",
    "    torch.save(model1.state_dict(), save_mode_path)\n",
    "    logging.info(\"save model to {}\".format(save_mode_path))\n",
    "    save_mode_path = os.path.join(snapshot_path, 'iter2_'+str(max_iterations)+'.pth')\n",
    "    torch.save(model2.state_dict(), save_mode_path)\n",
    "    logging.info(\"save model to {}\".format(save_mode_path))\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
