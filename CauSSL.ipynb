{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import random \n",
    "import time \n",
    "import logging \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "from utils.params import params \n",
    "from utils.losses import DiceLoss, softmax_mse_loss, softmax_kl_loss, l_correlation_cos_mean\n",
    "from networks.utils import BCP_net, get_current_consistency_weight, update_ema_variable\n",
    "from dataset.basedataset import BaseDataset\n",
    "from dataset.utils import TwoStreamBatchSampler, RandomGenerator, patients_to_slices\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ACDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data in used: 10.0%\n",
      "X.shape = torch.Size([24, 1, 256, 256])\n",
      "Y.shape = torch.Size([24, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "train_db = BaseDataset(\n",
    "    root_path= args.root_dir, \n",
    "    split= 'train', \n",
    "    transform= transforms.Compose([RandomGenerator(args.patch_size)])\n",
    ")\n",
    "\n",
    "val_db = BaseDataset(\n",
    "    root_path= args.root_dir, \n",
    "    split= 'val'\n",
    ")\n",
    "\n",
    "\n",
    "# split to labeled and unlabeled dataset \n",
    "labeled_slices = patients_to_slices(args.root_dir, args.label_num) \n",
    "label_ratio = round(labeled_slices / len(train_db), 1)* 100\n",
    "print(f'Number of labeled data in used: {label_ratio}%')\n",
    "labeled_idxs = list(range(0, labeled_slices) )\n",
    "unlabeled_idxs = list(range(labeled_slices ,len(train_db)))\n",
    "batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, args.batch_size, args.batch_size - args.labeled_bs)\n",
    "\n",
    "# Create dataloader\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(args.seed + worker_id)\n",
    " \n",
    "trainloader = DataLoader(train_db, batch_sampler= batch_sampler, num_workers= 4, pin_memory= True, worker_init_fn= worker_init_fn)\n",
    "valloader = DataLoader(val_db, batch_size= 1, shuffle= False, num_workers=1)\n",
    "\n",
    "# Check \n",
    "dataiter = iter(trainloader) \n",
    "sampled_batch = next(dataiter) \n",
    "volume_image, volume_label = sampled_batch['image'], sampled_batch['label']\n",
    "volume_image, volume_label = volume_image.cuda(), volume_label.cuda()\n",
    "labeled_volume_batch = volume_image[ : args.labeled_bs]\n",
    "unlabeled_volume_batch = volume_image[args.labeled_bs :]\n",
    "print(f'X.shape = {volume_image.shape}')\n",
    "print(f'Y.shape = {volume_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Vector(nn.Module): \n",
    "    \"\"\"\n",
    "    Implement: \n",
    "        - Initialize the linear transform matrix. G.shape = (ndim, ndim) \n",
    "        - Apply Gb to performce linear transform \n",
    "    Formula: \n",
    "        qb.shape = G * w --> (ndim, k)\n",
    "        w.shape = (ndim, k)\n",
    "        G.shape = (ndim, ndim) \n",
    "    \"\"\"\n",
    "    def __init__(self, ndim): \n",
    "        super(Linear_Vector, self).__init__()\n",
    "        self.ndim = ndim \n",
    "        self.params = Parameter(torch.Tensor(self.ndim, self.ndim)) # Linear transform matrix\n",
    "        self.ratio_init = 0.3 \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" \n",
    "        Initialize for Standard Scaler \n",
    "        mean = 0 \n",
    "        std = ratio_init \n",
    "        \"\"\" \n",
    "        for param in self.params: \n",
    "            param.data.normal_(0, self.ratio_init)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        result = torch.mm(self.params, x) # dot product  \n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "ndim = 64 \n",
    "k = 10 \n",
    "w1 = torch.randn(ndim, k) \n",
    "w2 = torch.randn(ndim, k)\n",
    "linear_trans1 = Linear_Vector(ndim= 64)\n",
    "\n",
    "q21 = linear_trans1(w2) \n",
    "print(q21.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Understand Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output.shape = torch.Size([24, 4, 256, 256])\n",
      "Type of output: torch.float32\n"
     ]
    }
   ],
   "source": [
    "base_model = BCP_net(in_chns= 1, class_num= args.num_classes) \n",
    "outputs = base_model(volume_image)\n",
    "print(f'Output.shape = {outputs.shape}')\n",
    "print(f'Type of output: {outputs.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss_fn = DiceLoss(n_classes= 4) \n",
    "def supervised_loss(outputs, target, alpha= 0.5): \n",
    "    \"\"\"\n",
    "    Comptute supervised loss for CauSSL (on labeled data only)\n",
    "    supervised = 0.5 ( CE + DICE )\n",
    "    \"\"\"\n",
    "    # Compute CELoss \n",
    "    target = target.long() \n",
    "    loss_ce = F.cross_entropy(outputs, target) \n",
    "\n",
    "    # Compute DiceLoss \n",
    "    loss_dice = dice_loss_fn(outputs, target.unsqueeze(1)) \n",
    "\n",
    "    loss = alpha * ( loss_ce + loss_dice)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1995, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check supervised loss \n",
    "outputs = base_model(volume_image)\n",
    "loss_sup = supervised_loss(outputs[: args.labeled_bs], volume_label[: args.labeled_bs])\n",
    "print(loss_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
