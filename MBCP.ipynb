{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d235d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9675135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 14:52:52.714495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-26 14:52:52.728520: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-26 14:52:52.732582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-26 14:52:52.743547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-26 14:52:53.821329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import logging\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from skimage.measure import label \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "# module \n",
    "from networks.net_factory import BCP_net\n",
    "from dataset.basedataset import ACDCDataset\n",
    "from dataset.utils import RandomGenerator, ACDC_patients_to_slices, TwoStreamBatchSampler\n",
    "from utils.params import params \n",
    "from utils.masks import generate_mask, random_mask, contact_mask\n",
    "from utils.losses import mix_loss\n",
    "from utils.valid2d import test_single_volume\n",
    "from networks.utils import save_net_opt, load_net_opt, load_net, get_current_consistency_weight, update_model_ema\n",
    "from MAE.maeLoss import reconstruction_loss\n",
    "from MAE.maskImage import mask_image\n",
    "\n",
    "# writer \n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eaf1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bea6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ACDC_2DLargestCC(segmentation):\n",
    "    batch_list = []\n",
    "    N = segmentation.shape[0]\n",
    "    for i in range(0, N):\n",
    "        class_list = []\n",
    "        for c in range(1, 4):\n",
    "            temp_seg = segmentation[i] #== c *  torch.ones_like(segmentation[i])\n",
    "            temp_prob = torch.zeros_like(temp_seg)\n",
    "            temp_prob[temp_seg == c] = 1\n",
    "            temp_prob = temp_prob.detach().cpu().numpy()\n",
    "            labels = label(temp_prob)          \n",
    "            if labels.max() != 0:\n",
    "                largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "                class_list.append(largestCC * c)\n",
    "            else:\n",
    "                class_list.append(temp_prob)\n",
    "        \n",
    "        n_batch = class_list[0] + class_list[1] + class_list[2]\n",
    "        batch_list.append(n_batch)\n",
    "\n",
    "    return torch.Tensor(batch_list).cuda()\n",
    "    \n",
    "def get_ACDC_masks(output, nms=0):\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    _, probs = torch.max(probs, dim=1)\n",
    "    if nms == 1:\n",
    "        probs = get_ACDC_2DLargestCC(probs)      \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41399da2",
   "metadata": {},
   "source": [
    "#### 1. BCP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5859cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(args, snapshot_path): \n",
    "    base_lr = args.base_lr\n",
    "    num_classes = args.num_classes\n",
    "    max_iterations = args.pretrain_iterations\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu \n",
    "    labeled_bs = args.labeled_bs \n",
    "\n",
    "    # Load data \n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(args.seed + worker_id) \n",
    "\n",
    "    label_sub_bs, unlabeled_sub_bs = int(args.labeled_bs / 2), int((args.batch_size - args.labeled_bs)/2) \n",
    "    db_train = ACDCDataset(base_dir= args.root_dir, split= 'train', \n",
    "                        transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "    db_val = ACDCDataset(base_dir= args.root_dir, split= 'val')\n",
    "    total_slices = len(db_train)\n",
    "    labeled_slices = ACDC_patients_to_slices(args.root_dir, args.label_num)\n",
    "    print(f'Total slices is: {total_slices}, labeled slices is: {labeled_slices}')\n",
    "    labeled_idxes = list(range(0, labeled_slices))\n",
    "    unlabeled_idxes = list(range(labeled_slices, total_slices))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxes, unlabeled_idxes, args.batch_size, args.batch_size - args.labeled_bs)\n",
    "\n",
    "    trainloader = DataLoader(db_train, batch_sampler= batch_sampler, num_workers= 4, pin_memory= True, worker_init_fn= worker_init_fn)\n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle= False, num_workers= 1)\n",
    "\n",
    "    # Model \n",
    "    model = BCP_net(in_chns=1, num_classes= num_classes)\n",
    "    optimizer = optim.SGD(model.parameters(), lr= base_lr, momentum= 0.9, weight_decay= 0.0001)\n",
    "\n",
    "    writer = SummaryWriter(snapshot_path + '/log')\n",
    "    logging.info('Start pre_training')\n",
    "    logging.info(f'{len(trainloader)} iterations per epoch')\n",
    "\n",
    "    model.train() \n",
    "    iter_num = 0 \n",
    "    max_epoch = max_iterations // len(trainloader) + 1 \n",
    "    best_performance = 0.0 \n",
    "    best_hd = 100 \n",
    "    iterator = tqdm(range(max_epoch), ncols= 70) \n",
    "    for _ in iterator: \n",
    "        for _, sampled_batch in enumerate(trainloader): \n",
    "            volume_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda() \n",
    "\n",
    "            # Create BCP image \n",
    "            img_a, img_b = volume_batch[: label_sub_bs], volume_batch[label_sub_bs : labeled_bs]\n",
    "            lab_a, lab_b = label_batch[: label_sub_bs], label_batch[label_sub_bs: labeled_bs]\n",
    "            img_mask, loss_mask =  generate_mask(img_a)\n",
    "            gt_mixl = lab_a * img_mask + lab_b * (1 - img_mask)\n",
    "\n",
    "            # Feed to model \n",
    "            net_input = img_a * img_mask + img_b * (1 - img_mask)\n",
    "            out_mixl = model(net_input, mode = 'seg')\n",
    "            loss_dice, loss_ce = mix_loss(out_mixl, lab_a, lab_b, loss_mask, u_weight= 1.0, unlab= True)\n",
    "\n",
    "            loss = (loss_dice + loss_ce ) / 2 \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            iter_num += 1 \n",
    "\n",
    "            writer.add_scalar('info/total loss', loss, iter_num)\n",
    "            writer.add_scalar('info/mix_dice', loss_dice, iter_num)\n",
    "            writer.add_scalar('info/mix_ce', loss_ce, iter_num)\n",
    "\n",
    "            logging.info(f'iteration: {iter_num}, loss: {loss}, mix_dice: {loss_dice}, mix_ce: {loss_ce}')\n",
    "            if iter_num % 20 == 0:\n",
    "                image = net_input[1, 0:1, :, :]  # shape = [1, H, W]\n",
    "                image = (image - image.min()) / (image.max() - image.min() + 1e-5)  # normalize to [0,1]\n",
    "                writer.add_image('pre_train/Mixed_Image', image, iter_num)\n",
    "                outputs = torch.argmax(torch.softmax(out_mixl, dim=1), dim=1, keepdim=True)\n",
    "                writer.add_image('pre_train/Mixed_Prediction', outputs[1, ...] * 50, iter_num)\n",
    "                labs = gt_mixl[1, ...].unsqueeze(0) * 50\n",
    "                writer.add_image('pre_train/Mixed_GroundTruth', labs, iter_num)\n",
    "\n",
    "            if iter_num > 0 and iter_num % 200 == 0:\n",
    "                model.eval()\n",
    "                metric_list = 0.0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    metric_i = test_single_volume(sampled_batch[\"image\"], sampled_batch[\"label\"], model, classes=num_classes)\n",
    "                    metric_list += np.array(metric_i)\n",
    "                metric_list = metric_list / len(db_val)\n",
    "                for class_i in range(num_classes-1):\n",
    "                    writer.add_scalar('info/val_{}_dice'.format(class_i+1), metric_list[class_i, 0], iter_num)\n",
    "                    writer.add_scalar('info/val_{}_hd95'.format(class_i+1), metric_list[class_i, 1], iter_num)\n",
    "\n",
    "                performance = np.mean(metric_list, axis=0)[0]\n",
    "                writer.add_scalar('info/val_mean_dice', performance, iter_num)\n",
    "\n",
    "                if performance > best_performance:\n",
    "                    best_performance = performance\n",
    "                    save_mode_path = os.path.join(snapshot_path, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))\n",
    "                    save_best_path = os.path.join(snapshot_path,'{}_best_model.pth'.format(args.model))\n",
    "                    save_net_opt(model, optimizer, save_mode_path)\n",
    "                    save_net_opt(model, optimizer, save_best_path)\n",
    "\n",
    "                logging.info('iteration %d : mean_dice : %f' % (iter_num, performance))\n",
    "                model.train()\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c29e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selftrain(args, pretrain_snapshot_path, snapshot_path): \n",
    "    # Extract params\n",
    "    base_lr = args.base_lr\n",
    "    num_classes = args.num_classes\n",
    "    max_iterations = args.selftrain_iterations\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu \n",
    "    pretrained_model = os.path.join(pretrain_snapshot_path, f'{args.model}_best_model.pth')\n",
    "    labeled_bs = args.labeled_bs\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(args.labeled_bs / 2), int((args.batch_size - args.labeled_bs) /2 )\n",
    "\n",
    "    # Load data \n",
    "    def worker_init_fn(worker_id): \n",
    "        random.seed(args.seed + worker_id)\n",
    "\n",
    "    db_train = ACDCDataset(base_dir= args.root_dir, split= 'train', \n",
    "                        transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "    db_val = ACDCDataset(base_dir= args.root_dir, split= 'val')\n",
    "    total_slices = len(db_train)\n",
    "    labeled_slices = ACDC_patients_to_slices(args.root_dir, args.label_num)\n",
    "    print(f'Total slices: {total_slices}, Labeled slices: {labeled_slices}')\n",
    "    labeled_idxs = list(range(0, labeled_slices))\n",
    "    unlabeled_idxs = list(range(labeled_slices, total_slices))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, args.batch_size, args.batch_size - args.labeled_bs)\n",
    "    trainloader = DataLoader(db_train, batch_sampler= batch_sampler, num_workers= 4, pin_memory= True)\n",
    "    valloader = DataLoader(db_val, batch_size= 1, shuffle= False, num_workers= 1)\n",
    "\n",
    "    # Model \n",
    "    model = BCP_net(in_chns=1, num_classes= 4)\n",
    "    ema_model = BCP_net(in_chns= 1, num_classes= 4, ema= True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr= base_lr, momentum= 0.9, weight_decay= 1e-4)\n",
    "    load_net(ema_model, pretrained_model)\n",
    "    load_net_opt(model, optimizer, pretrained_model)\n",
    "    logging.info(f'Loaded from {pretrained_model}')\n",
    "    writer = SummaryWriter(snapshot_path + '/log')\n",
    "    logging.info('Start self-training')\n",
    "    logging.info(f'{len(trainloader)} iterations per epoch')\n",
    "\n",
    "    model.train() \n",
    "    ema_model.train() \n",
    "    iter_num = 0 \n",
    "    max_epoch = max_iterations // len(trainloader) + 1 \n",
    "    best_performance = 0.0 \n",
    "    best_hd = 100.0 \n",
    "    iterator = tqdm(range(0, max_epoch), ncols= 70)\n",
    "    for _ in iterator: \n",
    "        for _, sampled_batch in enumerate(trainloader): \n",
    "            volume_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda() \n",
    "\n",
    "            # BCP augmentation \n",
    "            img_a, img_b = volume_batch[: labeled_sub_bs], volume_batch[labeled_sub_bs : labeled_bs]\n",
    "            lab_a, lab_b = label_batch[: labeled_sub_bs], label_batch[labeled_sub_bs : labeled_bs]\n",
    "            uimg_a, uimg_b = volume_batch[labeled_bs : labeled_bs + unlabeled_sub_bs], volume_batch[labeled_bs + unlabeled_sub_bs :]\n",
    "            ulab_a, ulab_b = label_batch[labeled_bs : labeled_bs + unlabeled_sub_bs], label_batch[labeled_bs + unlabeled_sub_bs :]\n",
    "            with torch.no_grad(): \n",
    "                pre_a = ema_model(uimg_a, mode = 'seg')\n",
    "                pre_b = ema_model(uimg_b, mode = 'seg')\n",
    "                plab_a = get_ACDC_masks(pre_a, nms= 1)\n",
    "                plab_b = get_ACDC_masks(pre_b, nms= 1)\n",
    "                img_mask, loss_mask = generate_mask(img_a)\n",
    "                unl_label = plab_a * img_mask + lab_a * (1 - img_mask) # TODO: Problem !! \n",
    "                l_label = lab_b * img_mask + plab_b * (1 - img_mask) #TODO: Problem !!! \n",
    "            consistency_weight = get_current_consistency_weight(args, iter_num // 150) # ADJUST\n",
    "            # ---------------------- Segmentation ----------------------------------- # \n",
    "            net_input_unl = uimg_a * img_mask + img_a * ( 1 - img_mask)\n",
    "            net_input_l = img_b * img_mask + uimg_b * (1 - img_mask)\n",
    "            out_unl = model(net_input_unl, mode='seg')\n",
    "            out_l = model(net_input_l, mode='seg')\n",
    "            unl_dice, unl_ce = mix_loss(out_unl, plab_a, lab_a, loss_mask, u_weight= args.u_weight, unlab= True)\n",
    "            l_dice, l_ce = mix_loss(out_l, lab_b, plab_b, loss_mask, u_weight= args.u_weight)\n",
    "\n",
    "            loss_ce = unl_ce + l_ce \n",
    "            loss_dice = unl_dice + l_dice \n",
    "            loss_bcp = (loss_ce + loss_dice) / 2 \n",
    "            # ----------------------------------- Reconstruction task --------------------------------- # \n",
    "            masked_img, mask = mask_image(volume_batch, block_size= 5, mask_ratio= 0.5)\n",
    "            masked_img = masked_img.cuda() \n",
    "            mask = mask.cuda() \n",
    "            \n",
    "            out_recon =  model(masked_img, mode = 'recon')\n",
    "            rec_loss = reconstruction_loss(out_recon, volume_batch, mask)\n",
    "            \n",
    "            # --------------------------------------- Backward -------------------------------------  # \n",
    "            loss = args.bcp_weight * loss_bcp \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            iter_num += 1 \n",
    "            update_model_ema(model, ema_model, 0.99)\n",
    "\n",
    "            writer.add_scalar('info/total_loss', loss, iter_num)\n",
    "            writer.add_scalar('info/mix_dice', loss_dice, iter_num)\n",
    "            writer.add_scalar('info/mix_ce', loss_ce, iter_num)\n",
    "            writer.add_scalar('info/consistency_weight', consistency_weight, iter_num)   \n",
    "            # writer.add_scalar('info/recon_loss', rec_loss.item(), iter_num)\n",
    "            # logging.info(f'iteration: {iter_num}, mix_dice: {loss_dice}, mix_ce: {loss_ce}, rec_loss: {rec_loss}')\n",
    "                \n",
    "            if iter_num % 20 == 0:\n",
    "                # Compare the reconstruct task \n",
    "                gt_image = volume_batch[1, 0:1].detach().cpu()\n",
    "                gt_image = (gt_image - gt_image.min()) / (gt_image.max() - gt_image.min())\n",
    "                writer.add_image('train/Reconstruction_GT', gt_image, iter_num)\n",
    "\n",
    "                # recon_image = out_recon[1, 0:1].detach().cpu()\n",
    "                # recon_image = (recon_image - recon_image.min()) / (recon_image.max() - recon_image.min() + 1e-5)\n",
    "                # writer.add_image('train/Reconstruction', recon_image, iter_num)\n",
    "\n",
    "                image = net_input_unl[1, 0:1, :, :]\n",
    "                writer.add_image('train/Un_Image', image, iter_num)\n",
    "                outputs = torch.argmax(torch.softmax(out_unl, dim=1), dim=1, keepdim=True)\n",
    "                writer.add_image('train/Un_Prediction', outputs[1, ...] * 50, iter_num)\n",
    "                labs = unl_label[1, ...].unsqueeze(0) * 50\n",
    "                writer.add_image('train/Un_GroundTruth', labs, iter_num)\n",
    "\n",
    "                image_l = net_input_l[1, 0:1, :, :]\n",
    "                writer.add_image('train/L_Image', image_l, iter_num)\n",
    "                outputs_l = torch.argmax(torch.softmax(out_l, dim=1), dim=1, keepdim=True)\n",
    "                writer.add_image('train/L_Prediction', outputs_l[1, ...] * 50, iter_num)\n",
    "                labs_l = l_label[1, ...].unsqueeze(0) * 50\n",
    "                writer.add_image('train/L_GroundTruth', labs_l, iter_num)\n",
    "\n",
    "            if iter_num > 0 and iter_num % 200 == 0:\n",
    "                model.eval()\n",
    "                metric_list = 0.0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    metric_i = test_single_volume(sampled_batch[\"image\"], sampled_batch[\"label\"], model, classes=num_classes)\n",
    "                    metric_list += np.array(metric_i)\n",
    "                metric_list = metric_list / len(db_val)\n",
    "                for class_i in range(num_classes-1):\n",
    "                    writer.add_scalar('info/val_{}_dice'.format(class_i+1), metric_list[class_i, 0], iter_num)\n",
    "                    writer.add_scalar('info/val_{}_hd95'.format(class_i+1), metric_list[class_i, 1], iter_num)\n",
    "\n",
    "                performance = np.mean(metric_list, axis=0)[0]\n",
    "                writer.add_scalar('info/val_mean_dice', performance, iter_num)\n",
    "\n",
    "                if performance > best_performance:\n",
    "                    best_performance = performance\n",
    "                    save_mode_path = os.path.join(snapshot_path, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))\n",
    "                    save_best_path = os.path.join(snapshot_path,'{}_best_model.pth'.format(args.model))\n",
    "                    torch.save(model.state_dict(), save_mode_path)\n",
    "                    torch.save(model.state_dict(), save_best_path)\n",
    "\n",
    "                logging.info('iteration %d : mean_dice : %f' % (iter_num, performance))\n",
    "                model.train()\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ef0509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: train: 1312 samples in total\n",
      "Mode: val: 20 samples in total\n",
      "Total slices is: 1312, labeled slices is: 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████▋| 90/91 [01:30<00:01,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: train: 1312 samples in total\n",
      "Mode: val: 20 samples in total\n",
      "Total slices: 1312, Labeled slices: 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████▉                            | 11/73 [00:30<02:50,  2.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m pretrain(args, pretrain_snapshot_path)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mselftrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_snapshot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselftrain_snapshot_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 91\u001b[0m, in \u001b[0;36mselftrain\u001b[0;34m(args, pretrain_snapshot_path, snapshot_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m iter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m     89\u001b[0m update_model_ema(model, ema_model, \u001b[38;5;241m0.99\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minfo/total_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo/mix_dice\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_dice, iter_num)\n\u001b[1;32m     93\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo/mix_ce\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_ce, iter_num)\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:378\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add scalar data to summary.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:371\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmake_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    373\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    374\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:24\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array or torch tensor are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:33\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_snapshot_path = 'modelBCP/pretrain'\n",
    "selftrain_snapshot_path = 'modelBCP/selftrain'\n",
    "for snapshot in [pretrain_snapshot_path, selftrain_snapshot_path]: \n",
    "    os.makedirs(snapshot, exist_ok= True)\n",
    "\n",
    "for handler in logging.root.handlers[:]: \n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename= 'modelBCP/log.txt',\n",
    "    level= logging.INFO, \n",
    "    format= '[%(asctime)s] %(message)s', \n",
    "    datefmt= '%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "# Log out configuration \n",
    "logging.info(\"========== Experiment Configuration ==========\")\n",
    "logging.info(f\"Pretrain Iterations  : {args.pretrain_iterations}\")\n",
    "logging.info(f\"Selftrain Iterations : {args.selftrain_iterations}\")\n",
    "logging.info(f\"Batch Size           : {args.batch_size}\")\n",
    "logging.info(f\"Labeled Batch Size   : {args.labeled_bs}\")\n",
    "logging.info(f\"Labelled Patients    : {args.label_num}\")\n",
    "logging.info(f\"Learning Rate        : {args.base_lr}\")\n",
    "logging.info(f\"Patch Size           : {args.patch_size}\")\n",
    "logging.info(f\"BCP Weight           : {args.bcp_weight}\")\n",
    "logging.info(f\"Reconstruction Weight: {args.recon_weight}\")\n",
    "logging.info(\"==============================================\")\n",
    "\n",
    "pretrain(args, pretrain_snapshot_path)\n",
    "selftrain(args, pretrain_snapshot_path, selftrain_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d59ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
