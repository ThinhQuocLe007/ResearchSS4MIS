{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:01:26.503894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-11 15:01:26.517046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-11 15:01:26.520931: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-11 15:01:26.532812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11 15:01:27.530151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from skimage.measure import label\n",
    "import sys \n",
    "import random \n",
    "import torch.backends.cudnn as cudnn\n",
    "# torch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# Module \n",
    "from dataset.basedataset import ACDCDataset\n",
    "from dataset.utils import RandomGenerator\n",
    "from utils.losses import DiceLoss\n",
    "from networks.net_factory import BCP_net\n",
    "from utils import masks\n",
    "from utils.valid2d import test_single_volume, test_single_volume_mean\n",
    "from networks.utils import save_net_opt, load_net, load_net_opt\n",
    "from networks.utils import update_model_ema\n",
    "\n",
    "# writer \n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('always')\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param \n",
    "from utils.params import params\n",
    "args = params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir= 'modelSDCL') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ACDC_LargestCC(segmentation):\n",
    "    class_list = []\n",
    "    for i in range(1, 4):\n",
    "        temp_prob = segmentation == i * torch.ones_like(segmentation)\n",
    "        temp_prob = temp_prob.detach().cpu().numpy()\n",
    "        labels = label(temp_prob)\n",
    "        # -- with 'try'\n",
    "        assert (labels.max() != 0)  # assume at least 1 CC\n",
    "        largestCC = labels == np.argmax(np.bincount(labels.flat)[1:]) + 1\n",
    "        class_list.append(largestCC * i)\n",
    "    acdc_largestCC = class_list[0] + class_list[1] + class_list[2]\n",
    "    return torch.from_numpy(acdc_largestCC).cuda()\n",
    "\n",
    "\n",
    "def get_ACDC_2DLargestCC(segmentation):\n",
    "    batch_list = []\n",
    "    N = segmentation.shape[0]\n",
    "    for i in range(0, N):\n",
    "        class_list = []\n",
    "        for c in range(1, 4):\n",
    "            temp_seg = segmentation[i]  # == c *  torch.ones_like(segmentation[i])\n",
    "            temp_prob = torch.zeros_like(temp_seg)\n",
    "            temp_prob[temp_seg == c] = 1\n",
    "            temp_prob = temp_prob.detach().cpu().numpy()\n",
    "            labels = label(temp_prob)\n",
    "            if labels.max() != 0:\n",
    "                largestCC = labels == np.argmax(np.bincount(labels.flat)[1:]) + 1\n",
    "                class_list.append(largestCC * c)\n",
    "            else:\n",
    "                class_list.append(temp_prob)\n",
    "\n",
    "        n_batch = class_list[0] + class_list[1] + class_list[2]\n",
    "        batch_list.append(n_batch)\n",
    "    batch_np = np.array(batch_list)\n",
    "    batch_tensor = torch.from_numpy(batch_np).float().cuda()\n",
    "    return batch_tensor \n",
    "\n",
    "\n",
    "def get_ACDC_masks(output, nms=0):\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    _, probs = torch.max(probs, dim=1)\n",
    "    if nms == 1:\n",
    "        probs = get_ACDC_2DLargestCC(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = DiceLoss(n_classes= 4)\n",
    "def mix_loss(output, img_l, patch_l, mask, l_weight=1.0, u_weight=0.5, unlab=False):\n",
    "    CE = nn.CrossEntropyLoss(reduction='none')\n",
    "    img_l, patch_l = img_l.type(torch.int64), patch_l.type(torch.int64)\n",
    "    output_soft = F.softmax(output, dim=1)\n",
    "    image_weight, patch_weight = l_weight, u_weight\n",
    "    if unlab:\n",
    "        image_weight, patch_weight = u_weight, l_weight\n",
    "    patch_mask = 1 - mask\n",
    "\n",
    "    loss_dice = dice_loss(output_soft, img_l.unsqueeze(1), mask.unsqueeze(1)) * image_weight\n",
    "    loss_dice += dice_loss(output_soft, patch_l.unsqueeze(1), patch_mask.unsqueeze(1)) * patch_weight\n",
    "    loss_ce = image_weight * (CE(output, img_l) * mask).sum() / (mask.sum() + 1e-16)\n",
    "    loss_ce += patch_weight * (CE(output, patch_l) * patch_mask).sum() / (patch_mask.sum() + 1e-16)  # loss = loss_ce\n",
    "    return loss_dice, loss_ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(args, snapshot_path): \n",
    "    writer.add_text('Phase','========================== Pretrain =============================', 0)\n",
    "    # Pretrain\n",
    "    num_clases = args.num_classes\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "    # Load data \n",
    "    db_val = ACDCDataset(base_dir= args.root_dir, split= 'val')\n",
    "\n",
    "    c_batch_size = 12 \n",
    "    trainset_lab_a = ACDCDataset(base_dir= args.root_dir, \n",
    "                                split= 'train_lab', \n",
    "                                transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "\n",
    "    trainset_lab_b = ACDCDataset(base_dir= args.root_dir, \n",
    "                            split= 'train_lab',\n",
    "                            reverse= True, \n",
    "                            transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "\n",
    "    train_loader_a = DataLoader(trainset_lab_a, batch_size= c_batch_size, shuffle= False, num_workers= 0, drop_last= True)\n",
    "    train_loader_b = DataLoader(trainset_lab_b, batch_size= c_batch_size, shuffle=False,num_workers= 0, drop_last= True)\n",
    "    val_loader = DataLoader(db_val, batch_size=1, shuffle= False, num_workers=0)\n",
    "\n",
    "\n",
    "    # Network\n",
    "    model1 = BCP_net(model= 'unet', in_chns=1, num_classes= num_clases)\n",
    "    model2 = BCP_net(model= 'ResUnet', in_chns=1, num_classes= num_clases) \n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr= 1e-3)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr= 1e-3)\n",
    "    logging.info('optim.Adam pre_train problem')\n",
    "\n",
    "    logging.info('Start pre-traininig')\n",
    "    logging.info(f'{len(train_loader_a)} iterations per epoch ')\n",
    "\n",
    "    model1.train() \n",
    "    model2.train() \n",
    "\n",
    "    iter_num = 0 \n",
    "    best_performance1 = 0.0 \n",
    "    best_performance2 = 0.0 \n",
    "    max_epoch = args.max_pretrain_epoch\n",
    "    iterator = tqdm(range(1, max_epoch), ncols= 70)\n",
    "    for epoch in iterator: \n",
    "        logging.info('\\n')\n",
    "        for step, ((img_a, lab_a), (img_b, lab_b)) in enumerate(zip(train_loader_a, train_loader_b)): \n",
    "            img_a, img_b, lab_a, lab_b = img_a.cuda(), img_b.cuda(), lab_a.cuda(), lab_b.cuda() \n",
    "\n",
    "            img_mask, loss_mask = masks.generate_mask(img_a) \n",
    "\n",
    "            # -- original \n",
    "            net_input = img_a * img_mask + img_b * (1- img_mask) \n",
    "            \n",
    "            out_mixl_1 = model1(net_input) \n",
    "            out_mixl_2 = model2(net_input)\n",
    "\n",
    "            # print(f'Input.shape = {net_input.shape}')\n",
    "            # print(f'Label_a.shape = {lab_a.shape}')\n",
    "            # print(f'Label_b.shape = {lab_b.shape}')\n",
    "            # print(f'Output.shape = {out_mixl_1.shape}')\n",
    "            loss_dice_1, loss_ce_1 = mix_loss(out_mixl_1, lab_a, lab_b, loss_mask, u_weight= 1.0, unlab= True)\n",
    "            loss_dice_2, loss_ce_2 = mix_loss(out_mixl_2, lab_a, lab_b,loss_mask, u_weight= 1.0, unlab= True) \n",
    "\n",
    "            loss1 = (loss_dice_1 + loss_ce_1) / 2 \n",
    "            loss2 = (loss_dice_2 + loss_ce_2) / 2 \n",
    "\n",
    "            optimizer1.zero_grad() \n",
    "            loss1.backward() \n",
    "            optimizer1.step() \n",
    "\n",
    "            optimizer2.zero_grad() \n",
    "            loss2.backward() \n",
    "            optimizer2.step() \n",
    "\n",
    "            iter_num += 1\n",
    "            writer.add_scalar('Loss/model1_total', loss1.item(), iter_num) \n",
    "            writer.add_scalar('Loss/model1_dice', loss_dice_1.item(), iter_num)\n",
    "            writer.add_scalar('Loss/model1_ce', loss_ce_1.item(), iter_num)\n",
    "            writer.add_scalar('Loss/model2_total', loss2.item(), iter_num)\n",
    "            writer.add_scalar('Loss/model2_dice', loss_dice_2.item(), iter_num)\n",
    "            writer.add_scalar('Loss/model2_ce', loss_ce_2.item(), iter_num)\n",
    "            logging.info(f'iterations: {iter_num} loss: {loss1} loss_dice: {loss_dice_1} loss_ce: {loss_ce_1}')\n",
    "        \n",
    "        if epoch >= 0 and epoch % 5 == 0: \n",
    "            model1.eval() \n",
    "            model2.eval() \n",
    "            metric_list_1 = 0.0 \n",
    "            metric_list_2 = 0.0 \n",
    "            for _, (img_val, lab_val) in tqdm(enumerate(val_loader), ncols= 70): \n",
    "                metric_i_1 = test_single_volume(img_val, lab_val, model1, classes= num_clases)\n",
    "                metric_i_2 = test_single_volume(img_val, lab_val, model2, classes= num_clases)\n",
    "\n",
    "                metric_list_1 += np.array(metric_i_1)\n",
    "                metric_list_2 += np.array(metric_i_2)\n",
    "            \n",
    "            metric_list_1 = metric_list_1 / len(db_val)\n",
    "            metric_list_2 = metric_list_2 / len(db_val)\n",
    "\n",
    "            performance_1 = np.mean(metric_list_1, axis= 0)[0] \n",
    "            performance_2 = np.mean(metric_list_2, axis= 0)[0] \n",
    "\n",
    "            if performance_1 > best_performance1: \n",
    "                best_performance1 = performance_1 \n",
    "\n",
    "                save_model_path = os.path.join(snapshot_path, f\"iter_{iter_num}_dice_{performance_1:.4f}.pth\")\n",
    "                save_best_path = os.path.join(snapshot_path, 'best_model_1.pth')  \n",
    "\n",
    "                save_net_opt(model1, optimizer1, save_model_path)          \n",
    "                save_net_opt(model1, optimizer1, save_best_path)\n",
    "            \n",
    "            if performance_2 > best_performance2: \n",
    "                best_performance2 = performance_2 \n",
    "\n",
    "                save_model_path = os.path.join(snapshot_path, f\"iter_{iter_num}_dice_{performance_2:.4f}.pth\")\n",
    "                save_best_path = os.path.join(snapshot_path, 'best_model_2.pth')  \n",
    "\n",
    "                save_net_opt(model2, optimizer2, save_model_path)          \n",
    "                save_net_opt(model2, optimizer2, save_best_path)\n",
    "            writer.add_scalar('Performance/UNet', performance_1.item(), iter_num)\n",
    "            writer.add_scalar('Performance/ResUnet', performance_2.item(), iter_num)\n",
    "            logging.info('iteration %d : mean_dice : %f, val_maxdice : %f' % (iter_num, performance_1, best_performance1))\n",
    "            logging.info('resnet iteration %d : mean_dice : %f, val_maxdice : %f' % (iter_num, performance_2, best_performance2))\n",
    "            model1.train()\n",
    "            model2.train() \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support self-train ----- NEED TO UNDERSTAND AGAIN !!!!!  \n",
    "def softmax_mse_loss(input_logits, target_logits):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_softmax = F.softmax(input_logits, dim=1)\n",
    "    # target_softmax = F.softmax(target_logits, dim=1)\n",
    "    mse_loss = (input_softmax - target_logits) ** 2\n",
    "    return mse_loss\n",
    "\n",
    "def to_one_hot(tensor, nClasses):\n",
    "    \"\"\" Input tensor : Nx1xHxW\n",
    "    :param tensor:\n",
    "    :param nClasses:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert tensor.max().item() < nClasses, 'one hot tensor.max() = {} < {}'.format(torch.max(tensor), nClasses)\n",
    "    assert tensor.min().item() >= 0, 'one hot tensor.min() = {} < {}'.format(tensor.min(), 0)\n",
    "\n",
    "    size = list(tensor.size())\n",
    "    assert size[1] == 1\n",
    "    size[1] = nClasses\n",
    "    one_hot = torch.zeros(*size)\n",
    "    if tensor.is_cuda:\n",
    "        one_hot = one_hot.cuda(tensor.device)\n",
    "    one_hot = one_hot.scatter_(1, tensor, 1)\n",
    "    return one_hot\n",
    "\n",
    "def get_XOR_region(mixout1, mixout2): \n",
    "    s1 = torch.softmax(mixout1, dim=1)\n",
    "    l1 = torch.argmax(s1, dim=1) \n",
    "    s2 = torch.softmax(mixout2, dim=1) \n",
    "    l2 = torch.argmax(s2, dim= 1) \n",
    "\n",
    "    diff_mask = (l1 != l2) \n",
    "    return diff_mask\n",
    "\n",
    "def mix_mse_loss(net3_output, img_l, patch_l, mask= None, l_weight=1.0, u_weight=0.5, unlab= False, diff_mask=None): \n",
    "    img_l, patch_l = img_l.type(torch.int64),patch_l.type(torch.int64) \n",
    "    image_weight, patch_weight = l_weight, u_weight\n",
    "    if unlab: \n",
    "        image_weight, patch_weight = u_weight, l_weight\n",
    "\n",
    "    patch_mask = 1 - mask \n",
    "    img_l_onehot = to_one_hot(img_l.unsqueeze(1), 4)\n",
    "    patch_l_onehot = to_one_hot(patch_l.unsqueeze(1), 4)\n",
    "\n",
    "    mse_loss = torch.mean(softmax_mse_loss(net3_output,img_l_onehot), dim=1)*mask * image_weight\n",
    "    mse_loss += torch.mean(softmax_mse_loss(net3_output, patch_l_onehot), dim= 1) * patch_mask * patch_weight\n",
    "\n",
    "    loss = torch.sum(diff_mask * mse_loss) / (torch.sum(diff_mask) + 1e-16)\n",
    "    return loss \n",
    "     \n",
    "voxel_kl_loss = nn.KLDivLoss(reduction='none')\n",
    "def mix_max_kl_loss(net3_output, img_l, patch_l, mask, l_weight=1.0, u_weight=0.5, unlab=False, diff_mask=None):\n",
    "    img_l, patch_l = img_l.type(torch.int64), patch_l.type(torch.int64)\n",
    "\n",
    "    image_weight, patch_weight = l_weight, u_weight\n",
    "    if unlab:\n",
    "        image_weight, patch_weight = u_weight, l_weight\n",
    "\n",
    "    patch_mask = 1 - mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        s1 = torch.softmax(net3_output, dim=1)\n",
    "        l1 = torch.argmax(s1, dim=1)\n",
    "        img_diff_mask = (l1 != img_l)\n",
    "        patch_diff_mask = (l1 != patch_l)\n",
    "\n",
    "        uniform_distri = torch.ones(net3_output.shape)\n",
    "        uniform_distri = uniform_distri.cuda()\n",
    "\n",
    "    kl_loss = torch.mean(voxel_kl_loss(F.log_softmax(net3_output, dim=1), uniform_distri),\n",
    "                         dim=1) * mask * img_diff_mask * image_weight\n",
    "    kl_loss += torch.mean(voxel_kl_loss(F.log_softmax(net3_output, dim=1), uniform_distri),\n",
    "                          dim=1) * patch_mask * patch_diff_mask * patch_weight\n",
    "\n",
    "    sum_diff = torch.sum(mask * img_diff_mask * diff_mask) + torch.sum(patch_mask * patch_diff_mask * diff_mask)\n",
    "    loss = torch.sum(diff_mask * kl_loss) / (sum_diff + 1e-16)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-train \n",
    "def selftrain(args, pre_snapshot_path, snapshot_path): \n",
    "    num_classes = args.num_classes\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "    # Load data \n",
    "    db_val = ACDCDataset(base_dir= args.root_dir, split= 'val')\n",
    "    c_batch_size = 12 \n",
    "    trainset_lab_a = ACDCDataset(base_dir= args.root_dir, \n",
    "                                split= 'train_lab', \n",
    "                                transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "    trainset_unlab_a = ACDCDataset(base_dir= args.root_dir, \n",
    "                                split= 'train_unlab', \n",
    "                                transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "\n",
    "    trainset_lab_b = ACDCDataset(base_dir= args.root_dir, \n",
    "                                split= 'train_lab', \n",
    "                                reverse= True, \n",
    "                                transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "    trainset_unlabel_b = ACDCDataset(base_dir= args.root_dir, \n",
    "                                    split= 'train_unlab', \n",
    "                                    reverse= True, \n",
    "                                    transform= transforms.Compose([RandomGenerator(args.patch_size)]))\n",
    "\n",
    "    lab_loader_a = DataLoader(trainset_lab_a, batch_size= c_batch_size, shuffle= False, num_workers= 0, drop_last= True)\n",
    "    lab_loader_b = DataLoader(trainset_lab_b, batch_size= c_batch_size, shuffle= False, num_workers= 0, drop_last= True) \n",
    "    unlab_loader_a = DataLoader(trainset_unlab_a, batch_size= c_batch_size, shuffle= False, num_workers=0, drop_last= True) \n",
    "    unlab_loader_b = DataLoader(trainset_unlabel_b, batch_size= c_batch_size, shuffle= False, num_workers=0, drop_last= True) \n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle= False, num_workers=1)\n",
    "\n",
    "    # Prepare model\n",
    "    pre_train_model1 = os.path.join(pre_snapshot_path, 'best_model_1.pth')\n",
    "    pre_train_model2 = os.path.join(pre_snapshot_path, 'best_model_2.pth')\n",
    "\n",
    "    model1 = BCP_net(model= 'unet', in_chns=1, num_classes= num_classes)\n",
    "    model2 = BCP_net(model= 'ResUnet', in_chns= 1, num_classes= num_classes)\n",
    "    ema_model = BCP_net(model= 'unet', in_chns=1, num_classes= num_classes, ema= True)\n",
    "\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr= 1e-3)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr= 1e-3)\n",
    "\n",
    "    load_net_opt(ema_model, optimizer1, pre_train_model1)\n",
    "    load_net_opt(model1, optimizer1, pre_train_model1)\n",
    "    load_net_opt(model2, optimizer2, pre_train_model2)\n",
    "    print(f'Load from: {pre_train_model1}')\n",
    "    logging.info(f'Load from {pre_snapshot_path}')\n",
    "\n",
    "    logging.info('Start self-training')\n",
    "    model1.train() \n",
    "    model2.train() \n",
    "    ema_model.train() \n",
    "\n",
    "    iter_num = 0 \n",
    "    best_performance_1 = 0.0 \n",
    "    best_performance_2 = 0.0 \n",
    "    best_performance_mean = 0.0 \n",
    "\n",
    "    max_epoch = args.max_seftrain_epoch \n",
    "    iterator = tqdm(range(1, max_epoch), ncols= 70) \n",
    "    print(f'Trainloader LAB: {len(lab_loader_a)}')\n",
    "    print(f'Trainloader UNLAB: {len(unlab_loader_a)}')\n",
    "    for epoch in iterator: \n",
    "        for step, ((img_a, lab_a), (img_b, lab_b), (unimg_a, unlab_a), (unimg_b, unlab_b)) in enumerate(zip(lab_loader_a, lab_loader_b, unlab_loader_a, unlab_loader_b)):\n",
    "            img_a, lab_a, img_b, lab_b, unimg_a, unlab_a, unimg_b, unlab_b = img_a.cuda(), lab_a.cuda(), img_b.cuda(), lab_b.cuda(), unimg_a.cuda(), unlab_a.cuda(), unimg_b.cuda(), unlab_b.cuda()\n",
    "\n",
    "            # Pseudo label        \n",
    "            with torch.no_grad(): \n",
    "                pre_a = ema_model(unimg_a)\n",
    "                pre_b = ema_model(unimg_b)\n",
    "                plab_a = get_ACDC_masks(pre_a, nms=1)\n",
    "                plab_b = get_ACDC_masks(pre_b, nms=1)\n",
    "\n",
    "                img_mask, loss_mask = masks.generate_mask(img_a)\n",
    "            \n",
    "            # BCP problem \n",
    "            net_input_l = unimg_a * img_mask + img_b*(1- img_mask)\n",
    "            net_input_unl = img_a * img_mask + unimg_b * ( 1 - img_mask)\n",
    "\n",
    "            out_l_1 =  model1(net_input_l)\n",
    "            out_unl_1 = model1(net_input_unl)\n",
    "            out_l_2 = model2(net_input_l)\n",
    "            out_unl_2 = model2((net_input_unl))\n",
    "            # TODO: CONSIDER PROBLEM HERE \n",
    "            l_dice_1, l_ce_1 = mix_loss(out_l_1, plab_a.long(), lab_b, loss_mask, u_weight= args.u_weight, unlab= True) \n",
    "            un_dice_1, un_ce_1 = mix_loss(out_unl_1, lab_a, plab_b.long(), loss_mask, u_weight= args.u_weight)\n",
    "\n",
    "            l_dice_2, l_ce_2 = mix_loss(out_l_2, plab_a.long(), lab_b, loss_mask, u_weight= args.u_weight, unlab= True) \n",
    "            un_dice_2, un_ce_2 = mix_loss(out_unl_2, lab_a, plab_b.long(), loss_mask, u_weight= args.u_weight) \n",
    "            \n",
    "            # Average loss \n",
    "            loss_dice_1 = l_dice_1 + un_dice_1\n",
    "            loss_ce_1 = l_ce_1 + un_ce_1\n",
    "            loss_dice_2 = l_dice_2 + un_dice_2 \n",
    "            loss_ce_2 = l_ce_2 + un_ce_2  \n",
    "\n",
    "            with torch.no_grad(): \n",
    "                diff_mask1 = get_XOR_region(out_l_1, out_l_2)\n",
    "                diff_mask_2 = get_XOR_region(out_unl_1, out_unl_2)\n",
    "            \n",
    "            net1_mse_loss_lab = mix_mse_loss(out_l_1, plab_a.long(), lab_b, loss_mask, unlab= True, diff_mask= diff_mask1)\n",
    "            net1_kl_loss_lab = mix_max_kl_loss(out_l_1, plab_a.long(), lab_b, loss_mask, unlab= True, diff_mask= diff_mask1)\n",
    "\n",
    "            net1_mse_loss_unlab = mix_mse_loss(out_unl_1, lab_a, plab_b.long(), loss_mask, diff_mask= diff_mask_2)\n",
    "            net1_kl_loss_unlab = mix_max_kl_loss(out_unl_1, lab_a, plab_b.long(), loss_mask, diff_mask= diff_mask_2)\n",
    "\n",
    "            net2_mse_loss_lab = mix_mse_loss(out_l_2, plab_a.long(), lab_b, loss_mask, unlab= True, diff_mask= diff_mask1)\n",
    "            net2_kl_loss_lab = mix_max_kl_loss(out_l_2, lab_a, plab_b.long(), loss_mask, unlab= True, diff_mask= diff_mask1)\n",
    "\n",
    "            net2_mse_loss_unlab = mix_mse_loss(out_unl_2, lab_a, plab_b.long(), loss_mask, diff_mask= diff_mask_2)\n",
    "            net2_kl_loss_unlab = mix_max_kl_loss(out_unl_2, lab_a, plab_b.long(), loss_mask, diff_mask= diff_mask_2)\n",
    "\n",
    "            loss_1 = (loss_dice_1 + loss_ce_1)/2 + 0.5 * (net1_mse_loss_lab + net1_mse_loss_unlab) + 0.05 * (net1_kl_loss_lab + net1_kl_loss_unlab)\n",
    "            loss_2 = (loss_dice_2 + loss_ce_2)/2 + 0.5 * (net2_mse_loss_lab + net2_mse_loss_unlab) + 0.05 * (net2_kl_loss_lab + net2_kl_loss_unlab)\n",
    "\n",
    "            optimizer1.zero_grad() \n",
    "            loss_1.backward()\n",
    "            optimizer1.step() \n",
    "\n",
    "            optimizer2.zero_grad() \n",
    "            loss_2.backward() \n",
    "            optimizer2.step() \n",
    "\n",
    "            iter_num += 1 \n",
    "            update_model_ema(model1, ema_model, 0.99)\n",
    "            \n",
    "\n",
    "            logging.info({\n",
    "                'iter': iter_num,\n",
    "                'mix_dice': loss_dice_1.item(),\n",
    "                'mix_ce': loss_ce_1.item(),\n",
    "                'mse_lab': net1_mse_loss_lab.item(),\n",
    "                'mse_unlab': net1_mse_loss_unlab.item(),\n",
    "                'kl_lab': net1_kl_loss_lab.item(),\n",
    "                'kl_unlab': net1_kl_loss_unlab.item()\n",
    "            })\n",
    "\n",
    "            if iter_num % 200 == 0:\n",
    "                with torch.no_grad(): \n",
    "                    model1.eval()\n",
    "                    model2.eval()\n",
    "                    metric_list = 0.0\n",
    "                    metric_list_2 = 0.0\n",
    "                    metric_list_mean = 0.0\n",
    "\n",
    "                    for _, (img_val, lab_val) in tqdm(enumerate(valloader), ncols=70):\n",
    "                        metric_i = test_single_volume(img_val, lab_val, model1, classes=num_classes)\n",
    "                        metric_i_2 = test_single_volume(img_val, lab_val, model2, classes=num_classes)\n",
    "                        metric_i_mean = test_single_volume_mean(img_val, lab_val, model1, model2, classes=num_classes)\n",
    "\n",
    "                        metric_list += np.array(metric_i)\n",
    "                        metric_list_2 += np.array(metric_i_2)\n",
    "                        metric_list_mean += np.array(metric_i_mean)\n",
    "\n",
    "                    metric_list = metric_list / len(db_val)\n",
    "                    metric_list_2 = metric_list_2 / len(db_val)\n",
    "                    metric_list_mean = metric_list_mean / len(db_val)\n",
    "\n",
    "                    performance_1 = np.mean(metric_list, axis=0)[0]\n",
    "                    performance_2 = np.mean(metric_list_2, axis=0)[0]\n",
    "                    performance_mean = np.mean(metric_list_mean, axis=0)[0]\n",
    "\n",
    "                    if performance_1 > best_performance_1:\n",
    "                        best_performance_1 = performance_1\n",
    "                        save_mode_path = os.path.join(snapshot_path,\n",
    "                                                        'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance_1, 4)))\n",
    "                        save_best_path = os.path.join(snapshot_path, 'best_model.pth')\n",
    "                        save_net_opt(model1, optimizer1, save_mode_path)\n",
    "                        save_net_opt(model1, optimizer1, save_best_path)\n",
    "\n",
    "                    if performance_2 > best_performance_2:\n",
    "                        best_performance_2 = performance_2\n",
    "                        save_mode_path = os.path.join(snapshot_path,\n",
    "                                                        'iter_{}_dice_{}_res.pth'.format(iter_num, round(best_performance_2, 4)))\n",
    "                        save_best_path = os.path.join(snapshot_path, 'best_model_res.pth')\n",
    "                        save_net_opt(model2, optimizer2, save_mode_path)\n",
    "                        save_net_opt(model2, optimizer2, save_best_path)\n",
    "\n",
    "                    if performance_mean > best_performance_mean:\n",
    "                        best_performance_mean = performance_mean\n",
    "\n",
    "                        save_mode_path1 = os.path.join(snapshot_path, 'iter_{}_dice_{}_v.pth'.format(iter_num, round(\n",
    "                            best_performance_mean, 4)))\n",
    "                        save_best_path1 = os.path.join(snapshot_path, 'best_model_v.pth')\n",
    "\n",
    "                        save_mode_path2 = os.path.join(snapshot_path, 'iter_{}_dice_{}_r.pth'.format(iter_num, round(\n",
    "                            best_performance_mean, 4)))\n",
    "                        save_best_path2 = os.path.join(snapshot_path, 'best_model_r.pth')\n",
    "\n",
    "                        save_net_opt(model1, optimizer1, save_mode_path1)\n",
    "                        save_net_opt(model1, optimizer1, save_best_path1)\n",
    "\n",
    "                        save_net_opt(model2, optimizer2, save_mode_path2)\n",
    "                        save_net_opt(model2, optimizer2, save_best_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: val: 20 samples in total\n",
      "Mode: train_lab: 136 samples in total\n",
      "Mode: train_lab: 136 samples in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.61it/s]                | 4/40 [03:22<30:27, 50.76s/it]\n",
      "20it [00:12,  1.60it/s]                | 9/40 [07:50<26:44, 51.75s/it]\n",
      "20it [00:13,  1.54it/s]               | 14/40 [12:20<22:45, 52.52s/it]\n",
      "20it [00:12,  1.60it/s]               | 19/40 [16:51<18:25, 52.64s/it]\n",
      "20it [00:12,  1.58it/s]█▊             | 24/40 [21:20<14:01, 52.62s/it]\n",
      "20it [00:12,  1.57it/s]█████▉         | 29/40 [25:51<09:39, 52.71s/it]\n",
      "20it [00:12,  1.55it/s]██████████     | 34/40 [30:23<05:17, 52.94s/it]\n",
      "20it [00:12,  1.57it/s]██████████████▏| 39/40 [34:55<00:52, 52.82s/it]\n",
      "100%|█████████████████████████████████| 40/40 [36:00<00:00, 54.00s/it]\n"
     ]
    }
   ],
   "source": [
    "if args.deterministic:\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    # cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "pretrain_snapshot_path = 'modelSDCL/pretrain' \n",
    "selftrain_snapshot_path = 'modelSDCL/selftrain'\n",
    "for snapshot_path in [pretrain_snapshot_path,selftrain_snapshot_path]: \n",
    "    if not os.path.exists(snapshot_path): \n",
    "        os.makedirs(snapshot_path)\n",
    "        \n",
    "logging.basicConfig(\n",
    "    filename= 'modelSDCL/log.txt', \n",
    "    level= logging.INFO, \n",
    "    format= '[%(asctime)s.%(msecs)03d] %(message)s', \n",
    "    datefmt= '%H:%M:%S'\n",
    "\n",
    ")\n",
    "pre_train(args, pretrain_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: val: 20 samples in total\n",
      "Mode: train_lab: 136 samples in total\n",
      "Mode: train_unlab: 1176 samples in total\n",
      "Mode: train_lab: 136 samples in total\n",
      "Mode: train_unlab: 1176 samples in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lequocthinh/Desktop/pythonCode/SSL4MIS/BCP/Medical_Seg_Semi/networks/utils.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(str(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from: modelSDCL/pretrain/best_model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainloader LAB: 113\n",
      "Trainloader UNLAB: 980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▋                               | 1/20 [01:58<37:35, 118.71s/it]/home/lequocthinh/miniconda3/envs/DeepLearning/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10269) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  self.pid = os.fork()\n",
      "20it [00:19,  1.01it/s]\n",
      "20it [00:20,  1.01s/it]               | 3/20 [06:17<35:32, 125.42s/it]\n",
      "20it [00:19,  1.00it/s]               | 5/20 [10:34<31:36, 126.41s/it]\n",
      "20it [00:18,  1.10it/s]               | 7/20 [14:52<27:28, 126.81s/it]\n",
      "20it [00:19,  1.03it/s]               | 8/20 [17:11<26:07, 130.58s/it]\n",
      " 40%|█████████████▏                   | 8/20 [19:24<29:06, 145.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     17\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(snapshot_path)\n\u001b[1;32m     19\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\n\u001b[1;32m     20\u001b[0m     filename\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelSDCL/log.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     21\u001b[0m     level\u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mINFO, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mselftrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_snapshot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselftrain_snapshot_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m, in \u001b[0;36mselftrain\u001b[0;34m(args, pre_snapshot_path, snapshot_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrainloader UNLAB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unlab_loader_a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m iterator: \n\u001b[0;32m---> 63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43munimg_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43munimg_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlab_loader_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_loader_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_loader_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_loader_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munimg_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munimg_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_b\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munimg_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munimg_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlab_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Pseudo label        \u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/pythonCode/SSL4MIS/BCP/Medical_Seg_Semi/dataset/basedataset.py:54\u001b[0m, in \u001b[0;36mACDCDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label}\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform: \n\u001b[0;32m---> 54\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m image_, label_ \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_, label_\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/pythonCode/SSL4MIS/BCP/Medical_Seg_Semi/dataset/utils.py:41\u001b[0m, in \u001b[0;36mRandomGenerator.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     38\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m random_rot_flip(image, label)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m: \n\u001b[0;32m---> 41\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_rotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Zoom image to -> [256,256]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m x,y \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/pythonCode/SSL4MIS/BCP/Medical_Seg_Semi/dataset/utils.py:27\u001b[0m, in \u001b[0;36mrandom_rotate\u001b[0;34m(image, label)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_rotate\u001b[39m(image, label):\n\u001b[1;32m     26\u001b[0m     angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m) \n\u001b[0;32m---> 27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     label \u001b[38;5;241m=\u001b[39m rotate(label,angle, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reshape\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m )\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/scipy/ndimage/_interpolation.py:984\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(input, angle, axes, reshape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    980\u001b[0m output \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_get_output(output, input_arr, shape\u001b[38;5;241m=\u001b[39moutput_shape,\n\u001b[1;32m    981\u001b[0m                                  complex_output\u001b[38;5;241m=\u001b[39mcomplex_output)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 984\u001b[0m     \u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrot_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m                     \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefilter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# If ndim > 2, the rotation is applied over all the planes\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# parallel to axes\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     planes_coord \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;241m*\u001b[39m[[\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(img_shape[ax])\n\u001b[1;32m    991\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim)])\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/scipy/ndimage/_interpolation.py:626\u001b[0m, in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    623\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[38;5;241m/\u001b[39mmatrix, output, order,\n\u001b[1;32m    624\u001b[0m                          mode, cval, npad, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometric_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.deterministic:\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    # cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "pretrain_snapshot_path = 'modelSDCL/pretrain' \n",
    "selftrain_snapshot_path = 'modelSDCL/selftrain'\n",
    "for snapshot_path in [pretrain_snapshot_path,selftrain_snapshot_path]: \n",
    "    if not os.path.exists(snapshot_path): \n",
    "        os.makedirs(snapshot_path)\n",
    "        \n",
    "logging.basicConfig(\n",
    "    filename= 'modelSDCL/log.txt', \n",
    "    level= logging.INFO, \n",
    "    format= '[%(asctime)s.%(msecs)03d] %(message)s', \n",
    "    datefmt= '%H:%M:%S'\n",
    "\n",
    ")\n",
    "selftrain(args, pretrain_snapshot_path, selftrain_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
